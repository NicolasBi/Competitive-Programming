# Analyse
- Histogrammes pour toutes les colonnes (si csv)
- Distribution des classes
- Visualisation pour voir s'il y a des clusters
- Clustering


# Preprocessing
- Données circulaires
- One-Hot-Encoding
- Normalize (en sauvegardant les paramètres pour dénormaliser)

# Entraînement de model
- Cross-Validation pour Scikit-Learn et PyTorch
- Réflechir à de la data augmentation (cacher une partie de l'info par exemple)
- Plot automatique des gradients et loss (cf vizdom)


# Evaluation de model
## Analyse des sorties
- Regarder la distribution des classes prédites et le degré de prédiction
- Analyser la répartition des probabilitées (mean & std) par classe
-


# Random ideas
- Regarder le semi-supervisé pour récupérer exploiter un max de données
