{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing dans Python\n",
    "\n",
    "### Multiprocessing ?\n",
    "\n",
    "Multiprocessing (plusiers threads dans cas de [Python](https://docs.python.org/3.5/library/multiprocessing.html#module-multiprocessing.dummy]) ) permet de paralliser l'execution des scripts/programs sequentiels. \n",
    "Parallelisation peut d'accelerer l'execution des programs.\n",
    "\n",
    "### Machine Learning en parallel\n",
    "(https://www.slideshare.net/ogrisel/strategies-and-tools-for-parallel-machine-learning-in-python)\n",
    "Les slides parlent de trois cas d'utilisation differentes :\n",
    "1. Evaluation d'un modéle\n",
    "2. Selection d'un modéle\n",
    "3. Apprentissage d'un modéle tel que RandomForest, Boosting, Bagging\n",
    "\n",
    "Toutes les choses possible à paralliser dans Machine Learning 'classique' avec Sklearn ont déjà une bon implementation. Le seul chose à faire est de definir le keyword `n_jobs=n` ou n est le nombre des cores disponibles. Par exemple, une [GridSearch](http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html) avec plusieurs modèles et paramètres peut d'étre lancer parallerement avec `GridSearchCV(pipe, param_grid, ... , n_jobs=8)`.\n",
    "\n",
    "### Réseau de neurones en parallel\n",
    "*Adrian et les autres fans du deep, n'hesite pas corriger si je dis les betises !*\n",
    "Dans le cas des réseau de neurones, l'execution est fait souvent sur les GPUs. L'execution peut d'étre partager entre plusieurs GPUs, il y a méme une [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html) dans le doc du PyTorch. Je ne sais pas, si on aura plusieurs GPUs dans le hachathon ? \n",
    "\n",
    "### Prétraitement et posttraitement en parallel\n",
    "C'est ici, où on aura besoin definir nous méme le parallelism. Il existe une API facile dans Python : `multiprocessing.Pool` (un [tutoriel](http://chriskiehl.com/article/parallelism-in-one-line/) bon). Il y a aussi beaucoup les implementations plus haut niveau, comme [Dask](https://dask.org/). Voila une petit exemple de tout les deux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ajoute le package local `src` pour pouvoir l'utiliser dans un notebook\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.data.malimg import load_malimg\n",
    "\n",
    "import time\n",
    "\n",
    "X,_ = load_malimg() # PREPROCESSING example, so take only labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def long_feature_engineering_task(line):\n",
    "    \"\"\" performs a FAKE transformation line by line (supposed to be slow so it's not vectorized)\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    line : array like\n",
    "        A list full of numbers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new (fake) field \n",
    "    \"\"\"\n",
    "    line_str = [str(elem) for elem in line]\n",
    "    line_str_extended = line_str * 4\n",
    "    big_str = '_'.join(line_str_extended)\n",
    "    counter = Counter(big_str)\n",
    "    return np.array(counter.most_common(3)).astype(int)[:,1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sans parallelism\n",
    "Le fake prétraitement sans parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.18769645690918\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_column = []\n",
    "for i in range(X.shape[0]):\n",
    "    result_column.append(long_feature_engineering_task(X[i, :]))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hundred_lines_job(indexes):\n",
    "    \"\"\" Job started by threadpool.\n",
    "    \n",
    "        Executes feature engineering for a hundred lines.\n",
    "    \"\"\"\n",
    "    return [long_feature_engineering_task(X[idx,:]) for idx in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.675493478775024\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n = int(X.shape[0] / 100)\n",
    "job_indexes = np.array_split(np.arange(X.shape[0]), n) # indexes for each job\n",
    "\n",
    "pool = ThreadPool()\n",
    "result = pool.map(hundred_lines_job, job_indexes)\n",
    "result_column = np.concatenate(result).ravel()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.716754913330078\n"
     ]
    }
   ],
   "source": [
    "from dask import compute, delayed\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n = int(X.shape[0] / 100)\n",
    "job_indexes = np.array_split(np.arange(X.shape[0]), n) # indexes for each job\n",
    "\n",
    "jobs = [delayed(hundred_lines_job)(indexes) for indexes in job_indexes]\n",
    "result = compute(*jobs, scheduler='processes')\n",
    "result_column = np.concatenate(result).ravel()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "J'ai voulu faire du pub pour le bibliotheque Dask, mais vu que c'est plus lente que pure Python dans cette tache, peut étre c'est mieux aller avec `from multiprocessing import Pool` simple :D. Dask est bon, si on a beaucoup des donnees, qui ne tient pas en memoire (alternative pour Spark etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
